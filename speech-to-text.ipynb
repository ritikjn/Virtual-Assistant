## Import all the libraries
import torch
import librosa
import numpy as np
import soundfile as sf
from scipy.io import wavfile
from IPython.display import Audio
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer


## Load tokenizer and model
#Now that we have successfully imported all the libraries, let’s load our tokenizer and the model. 
tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")


## Select a sample audio
file_name = 'my-audio.wav'
Audio(file_name)
## Adjust sample rate and output
#We load the audio file and check the sample rate and total time.
#For our input audio the Sampling rate comes out to be around 44100 Hz.
#We need to convert the sample rate to 16000 Hz as Facebook’s model accepts the sampling rate at this range, this is done using Librosa.
data = wavfile.read(file_name)
framerate = data[0]
sounddata = data[1]
time = np.arange(0,len(sounddata))/framerate
input_audio, _ = librosa.load(file_name, sr=16000)


#Finally, the input audio is fed to the tokenizer which is then processed by the model.
input_values = tokenizer(input_audio, return_tensors="pt").input_values
logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = tokenizer.batch_decode(predicted_ids)[0]
transcription
